{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Files in the folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_US_Qinglu.ipynb  data_WHO_Qinglu.ipynb  my_free_doc.txt\r\n",
      "data_US_shusma.ipynb  fda_database_doc.txt   Products.txt\r\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version:  3.8.10 (default, Jun  4 2021, 15:09:15) \n",
      "[GCC 7.5.0]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(\"Python version: \", sys.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total lines in the file:  44873\n",
      "Sample of drug names:  ['rytary', 'tenofovir alafenamide fumarate', 'serzone', 'butenafine hydrochloride', 'numorphan', 'hydromox', 'pioglitazone', 'betapace af', 'rezipas', 'sectral']\n",
      "number of drugs:  5583\n"
     ]
    }
   ],
   "source": [
    "lst = set()\n",
    "formulation_set = set()\n",
    "avoided_formulation = [\"inject\", \"im-iv\", \"intravenous\", \"intra-articular\", \"intramuscular\"]\n",
    "\n",
    "def isAcceptedDrugFormulation (formulation):\n",
    "    for avoided_form in avoided_formulation:\n",
    "        if formulation.find(avoided_form) >= 0:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "with open (\"Products.txt\", \"r\") as f:\n",
    "    f.readline()\n",
    "    c = 0\n",
    "    for line in f:\n",
    "        line_lst = line.split(\"\\t\")\n",
    "        formulation = line_lst[2].lower()\n",
    "        drugname = line_lst[5].lower()\n",
    "        if isAcceptedDrugFormulation(formulation):\n",
    "            lst.add(drugname)\n",
    "            formulation_set.add(formulation)\n",
    "        c += 1\n",
    "lst = list(lst)\n",
    "print(\"total lines in the file: \", c)\n",
    "print(\"Sample of drug names: \", list(lst)[:10])\n",
    "print(\"number of drugs: \", len(lst))\n",
    "# print(\"Sample of included formulation: \", list(formulation_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate numpy array of drugs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Provide list of drug names in numpy in case my teammate needs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5583,)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "drugset_np = np.array(list(lst))\n",
    "drugset_np.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5583\n"
     ]
    }
   ],
   "source": [
    "print(len(lst))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature generation step\n",
    "four types of error\n",
    "1. omission (missing one letter)\n",
    "2. transpose (order of two neighbor letters are incorrect)\n",
    "3. replacement\n",
    "a. vow-to-vow replacement\n",
    "b. consonant-to-consonant replacement\n",
    "4. insertion (insert random letter)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# omission 1 character\n",
    "\n",
    "def omission(drug, i):\n",
    "    if i == 0:\n",
    "        return drug[i+1:]\n",
    "    elif i == len(drug)-1:\n",
    "        return drug[:i]\n",
    "    else:\n",
    "        return drug[:i] + drug[i+1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bcdef\n",
      "acdef\n",
      "abdef\n",
      "abcef\n",
      "abcdf\n",
      "abcde\n"
     ]
    }
   ],
   "source": [
    "drug = \"abcdef\"\n",
    "for i in range(len(drug)):\n",
    "    print(omission (drug, i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transpose(drug, i):\n",
    "    if i >= len(drug)-1-1:\n",
    "        i = len(drug)-2\n",
    "        return drug[:i] + drug[i+1] +drug[i]\n",
    "    else:\n",
    "        return drug[:i] + drug[i+1] +drug[i] + drug[i+2:]\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "bacdef\n",
      "1\n",
      "acbdef\n",
      "2\n",
      "abdcef\n",
      "3\n",
      "abcedf\n",
      "4\n",
      "abcdfe\n",
      "5\n",
      "abcdfe\n"
     ]
    }
   ],
   "source": [
    "drug = \"abcdef\"\n",
    "for i in range(len(drug)):\n",
    "    print(i)\n",
    "    print(transpose(drug, i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'e', 'i', 'o', 'u']\n",
      "['b', 'c', 'd', 'f', 'g', 'h', 'j', 'k', 'l', 'm', 'n', 'p', 'q', 'r', 's', 't', 'v', 'w', 'x', 'y', 'z']\n",
      "['a', 'e', 'i', 'o', 'u', 'b', 'c', 'd', 'f', 'g', 'h', 'j', 'k', 'l', 'm', 'n', 'p', 'q', 'r', 's', 't', 'v', 'w', 'x', 'y', 'z']\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "VOWELS = list(\"aeiou\")\n",
    "CONSONANTS = list(\"bcdfghjklmnpqrstvwxyz\")\n",
    "ALL_LETTERS = VOWELS + CONSONANTS\n",
    "\n",
    "print(VOWELS)\n",
    "print(CONSONANTS)\n",
    "print(ALL_LETTERS)\n",
    "def replace(drug, i):\n",
    "    c = drug[i].lower()\n",
    "    if c in VOWELS:\n",
    "        r = random.choice(VOWELS)\n",
    "    elif c in CONSONANTS:\n",
    "        r = random.choice(CONSONANTS)\n",
    "    else:\n",
    "        r = random.choice(ALL_LETTERS)\n",
    "    ans = drug[:i] + r + drug[i+1:]\n",
    "    if ans == drug:\n",
    "        replace(drug, i)\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "abc__def\n",
      "1\n",
      "apc__def\n",
      "2\n",
      "abb__def\n",
      "3\n",
      "abcm_def\n",
      "4\n",
      "abc_ndef\n",
      "5\n",
      "abc__jef\n",
      "6\n",
      "abc__daf\n",
      "7\n",
      "abc__def\n"
     ]
    }
   ],
   "source": [
    "drug = \"abc__def\"\n",
    "for i in range(len(drug)):\n",
    "    print(i)\n",
    "    print(replace(drug, i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'e', 'i', 'o', 'u', 'b', 'c', 'd', 'f', 'g', 'h', 'j', 'k', 'l', 'm', 'n', 'p', 'q', 'r', 's', 't', 'v', 'w', 'x', 'y', 'z']\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "ALL_LETTERS = VOWELS + CONSONANTS\n",
    "\n",
    "print(ALL_LETTERS)\n",
    "\n",
    "def insertion(drug, i):\n",
    "    r = random.choice(ALL_LETTERS)\n",
    "    return drug[:i] + r + drug[i:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sabcdef\n",
      "albcdef\n",
      "abhcdef\n",
      "abcgdef\n",
      "abcdjef\n",
      "abcdehf\n",
      "abcdefe\n"
     ]
    }
   ],
   "source": [
    "drug = \"abcdef\"\n",
    "for i in range(len(drug)+1):\n",
    "    print(insertion(drug, i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this feature can be used for unsupervised learning and transformer data intake\n",
    "def genUnsprvFeatPair(drug):\n",
    "    temp = [omission(drug, i) for i in range(len(drug))]\n",
    "    temp += [transpose(drug, i) for i in range(len(drug)-1)]\n",
    "    temp += [replace(drug, i) for i in range(len(drug))]\n",
    "    temp += [insertion(drug, i) for i in range(len(drug))]\n",
    "    return [(x, drug) for x in temp]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('rug name', 'drug name'), ('dug name', 'drug name'), ('drg name', 'drug name'), ('dru name', 'drug name'), ('drugname', 'drug name'), ('drug ame', 'drug name'), ('drug nme', 'drug name'), ('drug nae', 'drug name'), ('drug nam', 'drug name'), ('rdug name', 'drug name'), ('durg name', 'drug name'), ('drgu name', 'drug name'), ('dru gname', 'drug name'), ('drugn ame', 'drug name'), ('drug anme', 'drug name'), ('drug nmae', 'drug name'), ('drug naem', 'drug name'), ('qrug name', 'drug name'), ('dhug name', 'drug name'), ('dreg name', 'drug name'), ('drup name', 'drug name'), ('drugpname', 'drug name'), ('drug qame', 'drug name'), ('drug nume', 'drug name'), ('drug name', 'drug name'), ('drug namo', 'drug name'), ('fdrug name', 'drug name'), ('dlrug name', 'drug name'), ('dreug name', 'drug name'), ('druzg name', 'drug name'), ('drugm name', 'drug name'), ('drug pname', 'drug name'), ('drug ngame', 'drug name'), ('drug napme', 'drug name'), ('drug nampe', 'drug name')]\n"
     ]
    }
   ],
   "source": [
    "drug = \"drug name\"\n",
    "print(genUnsprvFeatPair(drug))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "reference: dictionary spreader: https://sparrow.dev/object-spread-operator-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this feature can be used for NLTK ML models (naive bayes, sklearn models)\n",
    "def genSprvFeatPair(drug):\n",
    "    temp = {f\"omission_{i}\":omission(drug, i) for i in range(len(drug))}\n",
    "    temp2 = {f\"transpose_{i}\": transpose(drug, i) for i in range(len(drug)-1)}\n",
    "    temp3 = {f\"replace_{i}\": replace(drug, i) for i in range(len(drug))}\n",
    "    temp4 = {f\"insertion_{i}\":insertion(drug, i) for i in range(len(drug))}\n",
    "    return [{**temp, **temp2, **temp3, **temp4}, drug]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'omission_0': 'rug', 'omission_1': 'dug', 'omission_2': 'drg', 'omission_3': 'dru', 'transpose_0': 'rdug', 'transpose_1': 'durg', 'transpose_2': 'drgu', 'replace_0': 'prug', 'replace_1': 'dcug', 'replace_2': 'drug', 'replace_3': 'druf', 'insertion_0': 'bdrug', 'insertion_1': 'dgrug', 'insertion_2': 'drqug', 'insertion_3': 'druig'}, 'drug']\n"
     ]
    }
   ],
   "source": [
    "#test\n",
    "drug = \"drug\"\n",
    "print(genSprvFeatPair(drug))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Total number of drug names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5583"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('oprelto', 'goprelto'), ('gprelto', 'goprelto'), ('gorelto', 'goprelto'), ('gopelto', 'goprelto'), ('goprlto', 'goprelto'), ('gopreto', 'goprelto'), ('goprelo', 'goprelto'), ('goprelt', 'goprelto'), ('ogprelto', 'goprelto'), ('gporelto', 'goprelto')]\n",
      "294741\n"
     ]
    }
   ],
   "source": [
    "# unsprv_dataset only first 100. convert the whole dataset would cause too much time? \n",
    "Unsprv_dataset = sum([genUnsprvFeatPair(drug) for drug in lst[:]], [])\n",
    "print(Unsprv_dataset[:10])\n",
    "print(len(Unsprv_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[{'omission_0': 'oprelto', 'omission_1': 'gprelto', 'omission_2': 'gorelto', 'omission_3': 'gopelto', 'omission_4': 'goprlto', 'omission_5': 'gopreto', 'omission_6': 'goprelo', 'omission_7': 'goprelt', 'transpose_0': 'ogprelto', 'transpose_1': 'gporelto', 'transpose_2': 'gorpelto', 'transpose_3': 'goperlto', 'transpose_4': 'goprleto', 'transpose_5': 'gopretlo', 'transpose_6': 'goprelot', 'replace_0': 'zoprelto', 'replace_1': 'guprelto', 'replace_2': 'gohrelto', 'replace_3': 'goptelto', 'replace_4': 'goprilto', 'replace_5': 'gopresto', 'replace_6': 'goprelyo', 'replace_7': 'goprelta', 'insertion_0': 'ygoprelto', 'insertion_1': 'geoprelto', 'insertion_2': 'gokprelto', 'insertion_3': 'gopsrelto', 'insertion_4': 'goprqelto', 'insertion_5': 'gopreklto', 'insertion_6': 'goprelxto', 'insertion_7': 'gopreltgo'}, 'goprelto'], [{'omission_0': 'ylet', 'omission_1': 'zlet', 'omission_2': 'zyet', 'omission_3': 'zylt', 'omission_4': 'zyle', 'transpose_0': 'yzlet', 'transpose_1': 'zlyet', 'transpose_2': 'zyelt', 'transpose_3': 'zylte', 'replace_0': 'bylet', 'replace_1': 'zslet', 'replace_2': 'zycet', 'replace_3': 'zylit', 'replace_4': 'zylem', 'insertion_0': 'czylet', 'insertion_1': 'zbylet', 'insertion_2': 'zyslet', 'insertion_3': 'zylhet', 'insertion_4': 'zyleat'}, 'zylet']]\n",
      "5583\n"
     ]
    }
   ],
   "source": [
    "supervised_dataset = [genSprvFeatPair(drug) for drug in lst]\n",
    "print(supervised_dataset[:2])\n",
    "print(len(supervised_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This concludes the data preparation phase."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data analysis section\n",
    "1. lst (measure distance)\n",
    "2. Unsupervised dataset (transformer)\n",
    "3. Supervised dataset (for NLTK ML package)\n",
    "\n",
    "\n",
    "======================================================================\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unsupervised testing\n",
    "1. Jaccard distance\n",
    "2. Edit distance\n",
    "3. Word2Vec (most similar)\n",
    "\n",
    "\n",
    "Supervised Machine Learning\n",
    "1. Naive bayes (NLTK)\n",
    "2. Linear regression (NLTK)\n",
    "3. Logistic regression (NLTK)\n",
    "4. Transformer\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unsupervised learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Jaccard distance\n",
    "\n",
    "reference: https://www.geeksforgeeks.org/correcting-words-using-nltk-in-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.metrics.distance import jaccard_distance\n",
    "from nltk.util import ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "def WordLenDiff(w1, w2):\n",
    "    return abs(len(w1) - len(w2))\n",
    "\n",
    "print(WordLenDiff(\"abc\", \"abcdef\"))\n",
    "print(WordLenDiff(\"abcde\", \"abc\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total unsupervised dataset length:  294741\n"
     ]
    }
   ],
   "source": [
    "print(\"Total unsupervised dataset length: \", len(Unsprv_dataset))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i:  0\n",
      "i:  1000\n",
      "i:  2000\n",
      "i:  3000\n",
      "i:  4000\n",
      "i:  5000\n",
      "i:  6000\n",
      "i:  7000\n",
      "i:  8000\n",
      "i:  9000\n",
      "i final:  9999\n",
      "Jaccard distance accuracy:   0.96\n",
      "CPU times: user 1min 38s, sys: 30.4 ms, total: 1min 38s\n",
      "Wall time: 1min 38s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import random\n",
    "random.shuffle(Unsprv_dataset)\n",
    "\n",
    "incorrect_words=[x[0] for x in Unsprv_dataset[:10000]]\n",
    "ground_truth_words=[x[1] for x in Unsprv_dataset[:10000]]\n",
    "# print(\"incorrect_words\", incorrect_words)\n",
    "# print(\"correct_words\", correct_words2)\n",
    "correct_words = lst\n",
    "\n",
    "# loop for finding correct spellings\n",
    "# based on jaccard distance\n",
    "# and printing the correct word\n",
    "\n",
    "n_acc = 0\n",
    "n_total = 0\n",
    "\n",
    "for i, word in enumerate(incorrect_words):\n",
    "    if i % 1000 == 0:\n",
    "        print(\"i: \", i)\n",
    "    temp = [(jaccard_distance(set(ngrams(word, 2)),\n",
    "                              set(ngrams(w, 2))),w)\n",
    "            for w in correct_words if WordLenDiff(w, word) <=3]\n",
    "    pred = sorted(temp, key = lambda val:val[0])[0][1]\n",
    "    n_total += 1\n",
    "    if pred == ground_truth_words[i]:\n",
    "        n_acc += 1\n",
    "print(\"i final: \", i)\n",
    "print(f\"Jaccard distance accuracy: {n_acc/n_total:6.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For jaccard distance, the full dataset was tested against the method and reached accuracy of 0.96%. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. edit distance \n",
    "https://www.geeksforgeeks.org/correcting-words-using-nltk-in-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.metrics.distance  import edit_distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i:  0\n",
      "sulfamethoxazole and trimethoprim and phenazopyridine hydrohcloride\n",
      "sulfamethoxazole and trimethoprim and phenazopyridine hydrochloride\n",
      "sulfamethoxazole and trimethoprim and phenazopyridine hydrochloride\n",
      "reeglan\n",
      "reglan\n",
      "reglan\n",
      "penicillin-x2\n",
      "penicillin-2\n",
      "penicillin-2\n",
      "uqasense\n",
      "quasense\n",
      "quasense\n",
      "methdylin\n",
      "methylin\n",
      "methylin\n",
      "delflex w/ dextrose 2.5% low magnesium low cacium in plastic container\n",
      "delflex w/ dextrose 2.5% low magnesium low calcium in plastic container\n",
      "delflex w/ dextrose 2.5% low magnesium low calcium in plastic container\n",
      "chlorothiazide\n",
      "chlorothiazide\n",
      "chlorothiazide\n",
      "loxapine sucinate\n",
      "loxapine succinate\n",
      "loxapine succinate\n",
      "diflorasone iacetate\n",
      "diflorasone diacetate\n",
      "diflorasone diacetate\n",
      "rfiampin and isoniazid\n",
      "rifampin and isoniazid\n",
      "rifampin and isoniazid\n",
      "0th iter: accuracy: 1.0\n",
      "i:  0\n",
      "bacitracin zinc andupolymyxin b sulfate\n",
      "bacitracin zinc and polymyxin b sulfate\n",
      "bacitracin zinc and polymyxin b sulfate\n",
      "reserpine and hydrochlorothiazide-50\n",
      "reserpine and hydrochlorothiazide-50\n",
      "reserpine and hydrochlorothiazide-50\n",
      "esaklith\n",
      "eskalith\n",
      "eskalith\n",
      "tersiba\n",
      "tresiba\n",
      "tresiba\n",
      "allegra-d 1 2hour allergy and congestion\n",
      "allegra-d 12 hour allergy and congestion\n",
      "allegra-d 12 hour allergy and congestion\n",
      "ticagrelir\n",
      "ticagrelor\n",
      "ticagrelor\n",
      "hydrocodone ibtartrate and homatropine methylbromide\n",
      "hydrocodone bitartrate and homatropine methylbromide\n",
      "hydrocodone bitartrate and homatropine methylbromide\n",
      "tofacitixib\n",
      "tofacitinib\n",
      "tofacitinib\n",
      "lzool\n",
      "lozol\n",
      "lozol\n",
      "barymo er\n",
      "arymo er\n",
      "arymo er\n",
      "1th iter: accuracy: 1.0\n",
      "i:  0\n",
      "valsatan and hydrochlorothiazide\n",
      "valsartan and hydrochlorothiazide\n",
      "valsartan and hydrochlorothiazide\n",
      "amlodipine bekylate; benazepril hydrochloride\n",
      "amlodipine besylate; benazepril hydrochloride\n",
      "amlodipine besylate; benazepril hydrochloride\n",
      "cartril\n",
      "cortril\n",
      "cartrol\n",
      "edaklinza\n",
      "daklinza\n",
      "daklinza\n",
      "aurovwela 24 fe\n",
      "aurovela 24 fe\n",
      "aurovela 24 fe\n",
      "prpilosec otc\n",
      "prilosec otc\n",
      "prilosec otc\n",
      "inpersol-zm w/ dextrose 4.25% in plastic cnotainer\n",
      "inpersol-zm w/ dextrose 4.25% in plastic container\n",
      "inpersol-zm w/ dextrose 4.25% in plastic container\n",
      "dacular\n",
      "acular\n",
      "acular\n",
      "motofn\n",
      "motofen\n",
      "motofen\n",
      "cesalmet\n",
      "cesamet\n",
      "cesamet\n",
      "2th iter: accuracy: 0.9\n",
      "i:  0\n",
      "xejanz\n",
      "xeljanz\n",
      "xeljanz\n",
      "triamterewe and hydrochlorothiazide\n",
      "triamterene and hydrochlorothiazide\n",
      "triamterene and hydrochlorothiazide\n",
      "buprenorphine hydrochloride and naloxone hdyrochloride \n",
      "buprenorphine hydrochloride and naloxone hydrochloride \n",
      "buprenorphine hydrochloride and naloxone hydrochloride \n",
      "rabacavir sulfate\n",
      "abacavir sulfate\n",
      "abacavir sulfate\n",
      "aldoril 2s\n",
      "aldoril 25\n",
      "aldoril 25\n",
      "hydera\n",
      "hydrea\n",
      "hydrea\n",
      "dsudafed 24 hour\n",
      "sudafed 24 hour\n",
      "sudafed 24 hour\n",
      "etlioz lq\n",
      "hetlioz lq\n",
      "hetlioz lq\n",
      "pravigard hpac (copackaged)\n",
      "pravigard pac (copackaged)\n",
      "pravigard pac (copackaged)\n",
      "laophyllin\n",
      "lanophyllin\n",
      "lanophyllin\n",
      "3th iter: accuracy: 1.0\n",
      "i:  0\n",
      "sulfadiazivne\n",
      "sulfadiazine\n",
      "sulfadiazine\n",
      "men\n",
      "amen\n",
      "amen\n",
      "arformoterol tatrrate\n",
      "arformoterol tartrate\n",
      "arformoterol tartrate\n",
      "monistat 7 combinationw pack\n",
      "monistat 7 combination pack\n",
      "monistat 7 combination pack\n",
      "allegra-d 12 hour allergy andcongestion\n",
      "allegra-d 12 hour allergy and congestion\n",
      "allegra-d 12 hour allergy and congestion\n",
      "nerlyx\n",
      "nerlynx\n",
      "nerlynx\n",
      "motrinib\n",
      "motrin ib\n",
      "motrin ib\n",
      "inpersol-zm w/ dextrose 1.5% in plastic contaixer\n",
      "inpersol-zm w/ dextrose 1.5% in plastic container\n",
      "inpersol-zm w/ dextrose 1.5% in plastic container\n",
      "stifate\n",
      "stimate\n",
      "stimate\n",
      "biphetaamine 20\n",
      "biphetamine 20\n",
      "biphetamine 20\n",
      "4th iter: accuracy: 1.0\n",
      "i final:  9\n",
      "Edit distance accuracy:   0.98\n",
      "CPU times: user 8.18 s, sys: 19.1 ms, total: 8.2 s\n",
      "Wall time: 8.18 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import random\n",
    "arr_lst = []\n",
    "for iter in range(5):\n",
    "    random.shuffle(Unsprv_dataset)\n",
    "    incorrect_words=[x[0] for x in Unsprv_dataset]\n",
    "    ground_truth_words=[x[1] for x in Unsprv_dataset]\n",
    "    # print(\"incorrect_words\", incorrect_words)\n",
    "    # print(\"correct_words\", correct_words2)\n",
    "    correct_words = lst\n",
    "\n",
    "    # loop for finding correct spellings\n",
    "    # based on jaccard distance\n",
    "    # and printing the correct word\n",
    "\n",
    "    n_acc = 0\n",
    "    n_total = 0\n",
    "\n",
    "    for i, word in enumerate(incorrect_words):\n",
    "        if i % 100 == 0:\n",
    "            print(\"i: \", i)                 \n",
    "        temp = [(edit_distance(word, w),w) for w in correct_words if WordLenDiff(w, word) <=3]\n",
    "        pred = sorted(temp, key = lambda val:val[0])[0][1]\n",
    "#         print(word)\n",
    "#         print(pred)\n",
    "#         print(ground_truth_words[i])\n",
    "        n_total += 1\n",
    "        if pred == ground_truth_words[i]:\n",
    "            n_acc += 1\n",
    "        if i>=500:\n",
    "            break\n",
    "    acc = n_acc/n_total\n",
    "    arr_lst.append(acc)\n",
    "    print(f\"{iter}th iter: accuracy: {acc}\")\n",
    "                 \n",
    "print(\"i final: \", i)\n",
    "print(f\"Edit distance accuracy: {sum(arr_lst)/len(arr_lst):6.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Edit distance accuracy:   0.9900\n"
     ]
    }
   ],
   "source": [
    "print(f\"Edit distance accuracy: {sum(arr_lst)/len(arr_lst):8.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the larger time consumed for each search in edit distance, instead that iterating total 290000 samples, we randomly pick 100 samples from the full list and test accuracy. We iterate the process for 5 times and average the accuracy. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. word2vec\n",
    "- reference to hw2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "word2vec accuracy is 0. likely because each drug name only occurs 1 time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i:  0\n",
      "i:  10\n",
      "i:  20\n",
      "i:  30\n",
      "i:  40\n",
      "i:  50\n",
      "i:  60\n",
      "i:  70\n",
      "i:  80\n",
      "i:  90\n",
      "number of matched:  0\n",
      "0th iter accuracy: 0.0\n",
      "i:  0\n",
      "i:  10\n",
      "i:  20\n",
      "i:  30\n",
      "i:  40\n",
      "i:  50\n",
      "i:  60\n",
      "i:  70\n",
      "i:  80\n",
      "i:  90\n",
      "number of matched:  0\n",
      "1th iter accuracy: 0.0\n",
      "i:  0\n",
      "i:  10\n",
      "i:  20\n",
      "i:  30\n",
      "i:  40\n",
      "i:  50\n",
      "i:  60\n",
      "i:  70\n",
      "i:  80\n",
      "i:  90\n",
      "number of matched:  1\n",
      "2th iter accuracy: 0.01\n",
      "i:  0\n",
      "i:  10\n",
      "i:  20\n",
      "i:  30\n",
      "i:  40\n",
      "i:  50\n",
      "i:  60\n",
      "i:  70\n",
      "i:  80\n",
      "i:  90\n",
      "number of matched:  0\n",
      "3th iter accuracy: 0.0\n",
      "i:  0\n",
      "i:  10\n",
      "i:  20\n",
      "i:  30\n",
      "i:  40\n",
      "i:  50\n",
      "i:  60\n",
      "i:  70\n",
      "i:  80\n",
      "i:  90\n",
      "number of matched:  0\n",
      "4th iter accuracy: 0.0\n",
      "CPU times: user 12min 52s, sys: 3min 34s, total: 16min 26s\n",
      "Wall time: 9min 52s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import random\n",
    "\n",
    "acc_w2v_lst = []\n",
    "for iter in range(5):\n",
    "    random.shuffle(Unsprv_dataset)\n",
    "    incorrect_words=[x[0] for x in Unsprv_dataset[:100]]\n",
    "    ground_truth_words=[x[1] for x in Unsprv_dataset[:100]]\n",
    "\n",
    "    n_acc = 0\n",
    "    n_total = 0\n",
    "    for i, w_incorrect in enumerate(incorrect_words):\n",
    "        if i % 10 == 0:\n",
    "            print(\"i: \", i)\n",
    "        w_correct = ground_truth_words[i]\n",
    "        lst_new = [w_incorrect] +lst\n",
    "        model = Word2Vec(sentences=[lst_new], min_count=1, workers=4, iter=10)\n",
    "        output = model.wv.most_similar(w_incorrect)[0][0]\n",
    "        n_total += 1\n",
    "        if w_correct == output:\n",
    "            n_acc+=1\n",
    "    print(\"number of matched: \", n_acc)\n",
    "    print(f\"{iter}th iter accuracy: {n_acc/n_total}\")\n",
    "    acc_w2v_lst.append(n_acc/n_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word2vector accuracy for best match:   0.0020\n"
     ]
    }
   ],
   "source": [
    "print(f\"word2vector accuracy for best match: {sum(acc_w2v_lst)/len(acc_w2v_lst):8.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i:  0\n",
      "i:  10\n",
      "i:  20\n",
      "i:  30\n",
      "i:  40\n",
      "i:  50\n",
      "i:  60\n",
      "i:  70\n",
      "i:  80\n",
      "i:  90\n",
      "number of matched:  0\n",
      "0th iter accuracy: 0.0\n",
      "i:  0\n",
      "i:  10\n",
      "i:  20\n",
      "i:  30\n",
      "i:  40\n",
      "i:  50\n",
      "i:  60\n",
      "i:  70\n",
      "i:  80\n",
      "i:  90\n",
      "number of matched:  0\n",
      "1th iter accuracy: 0.0\n",
      "i:  0\n",
      "i:  10\n",
      "i:  20\n",
      "i:  30\n",
      "i:  40\n",
      "i:  50\n",
      "i:  60\n",
      "i:  70\n",
      "i:  80\n",
      "i:  90\n",
      "number of matched:  0\n",
      "2th iter accuracy: 0.0\n",
      "i:  0\n",
      "i:  10\n",
      "i:  20\n",
      "i:  30\n",
      "i:  40\n",
      "i:  50\n",
      "i:  60\n",
      "i:  70\n",
      "i:  80\n",
      "i:  90\n",
      "number of matched:  0\n",
      "3th iter accuracy: 0.0\n",
      "i:  0\n",
      "i:  10\n",
      "i:  20\n",
      "i:  30\n",
      "i:  40\n",
      "i:  50\n",
      "i:  60\n",
      "i:  70\n",
      "i:  80\n",
      "i:  90\n",
      "number of matched:  0\n",
      "4th iter accuracy: 0.0\n",
      "CPU times: user 12min 50s, sys: 3min 31s, total: 16min 21s\n",
      "Wall time: 9min 55s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import random\n",
    "\n",
    "acc_w2v_lst = []\n",
    "for iter in range(5):\n",
    "    random.shuffle(Unsprv_dataset)\n",
    "    incorrect_words=[x[0] for x in Unsprv_dataset[:100]]\n",
    "    ground_truth_words=[x[1] for x in Unsprv_dataset[:100]]\n",
    "\n",
    "    n_acc = 0\n",
    "    n_total = 0\n",
    "    for i, w_incorrect in enumerate(incorrect_words):\n",
    "        if i % 10 == 0:\n",
    "            print(\"i: \", i)\n",
    "        w_correct = ground_truth_words[i]\n",
    "        lst_new = [w_incorrect] +lst\n",
    "        model = Word2Vec(sentences=[lst_new], min_count=1, workers=4, iter=10)\n",
    "        output = model.wv.most_similar(w_incorrect)[:5]\n",
    "        output = [x[0] for x in output]\n",
    "        n_total += 1\n",
    "        if w_correct in output:\n",
    "            n_acc+=1\n",
    "    print(\"number of matched: \", n_acc)\n",
    "    print(f\"{iter}th iter accuracy: {n_acc/n_total}\")\n",
    "    acc_w2v_lst.append(n_acc/n_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word2vec accuracy for top 5:   0.0000\n"
     ]
    }
   ],
   "source": [
    "print(f\"word2vec accuracy for top 5: {sum(acc_w2v_lst)/len(acc_w2v_lst):8.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word2Vec is not right tool to solve spelling error check issue. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Supervised machine learning models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size:  800\n",
      "length of features:  1000\n",
      "Train on train set and Accuracy using test set: \n",
      "0.0\n",
      "Accuracy using train set (this is against protocol in machine learning. train on train set and test on train set): \n",
      "1.0\n",
      "CPU times: user 1min, sys: 46.7 ms, total: 1min\n",
      "Wall time: 1min\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# up to 1st 1000 drugs \n",
    "import random\n",
    "random.shuffle(supervised_dataset)\n",
    "\n",
    "dataset = supervised_dataset[:1000]\n",
    "\n",
    "split = 0.80\n",
    "size = int(len(dataset)*split)\n",
    "print(\"size: \", size)\n",
    "\n",
    "print(\"length of features: \", len(dataset))\n",
    "train_set, test_set = dataset[:size], dataset[size:]\n",
    "\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "print(\"Train on train set and Accuracy using test set: \")\n",
    "print(nltk.classify.accuracy(classifier, test_set))\n",
    "print(\"Accuracy using train set (this is against protocol in machine learning. train on train set and test on train set): \")\n",
    "print(nltk.classify.accuracy(classifier, train_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above result can be explained by scarcity of drug spelling. in order to reach 100% match in drug names, ML model can't retrieve answer from training set because spelling is very distinct. Each drug name is in its own label (class). \n",
    "\n",
    "Alternatively, since drug name is limited vocabulary, all wrong spelling are saved, we can store all drug mis-spelling in the model parameters. It will product very good result given taht the model sees this mis-spelling in the past. Therefore, Model can be built from previous failures (misspellings).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pyspellchecker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PySpellChecker documentation\n",
    "\n",
    "https://pypi.org/project/pyspellchecker/\n",
    "\n",
    "from spellchecker import SpellChecker\n",
    "\n",
    "spell = SpellChecker()\n",
    "\n",
    "# find those words that may be misspelled\n",
    "misspelled = spell.unknown(['something', 'is', 'hapenning', 'here'])\n",
    "\n",
    "for word in misspelled:\n",
    "    # Get the one `most likely` answer\n",
    "    print(spell.correction(word))\n",
    "\n",
    "    # Get a list of `likely` options\n",
    "    print(spell.candidates(word))\n",
    "\n",
    "If the Word Frequency list is not to your liking, you can add additional text to generate a more appropriate list for your use case.\n",
    "\n",
    "\n",
    "from spellchecker import SpellChecker\n",
    "\n",
    "spell = SpellChecker()  # loads default word frequency list\n",
    "spell.word_frequency.load_text_file('./my_free_text_doc.txt')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spellchecker import SpellChecker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "spell = SpellChecker()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "anafwanil anafranil\n"
     ]
    }
   ],
   "source": [
    "incorrect_words=[x[0] for x in Unsprv_dataset[:100]]\n",
    "ground_truth_words=[x[1] for x in Unsprv_dataset[:100]]\n",
    "w1 = incorrect_words[0]\n",
    "w2 = ground_truth_words[0]\n",
    "print(w1, w2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500\n",
      "i:  0\n",
      "i:  10\n",
      "i:  20\n",
      "i:  30\n",
      "i:  40\n",
      "i:  50\n",
      "i:  60\n",
      "i:  70\n",
      "i:  80\n",
      "i:  90\n",
      "i:  100\n",
      "i:  110\n",
      "i:  120\n",
      "i:  130\n",
      "i:  140\n",
      "i:  150\n",
      "i:  160\n",
      "i:  170\n",
      "i:  180\n",
      "i:  190\n",
      "i:  200\n",
      "i:  210\n",
      "i:  220\n",
      "i:  230\n",
      "i:  240\n",
      "i:  250\n",
      "i:  260\n",
      "i:  270\n",
      "i:  280\n",
      "i:  290\n",
      "i:  300\n",
      "i:  310\n",
      "i:  320\n",
      "i:  330\n",
      "i:  340\n",
      "i:  350\n",
      "i:  360\n",
      "i:  370\n",
      "i:  380\n",
      "i:  390\n",
      "i:  400\n",
      "i:  410\n",
      "i:  420\n",
      "i:  430\n",
      "i:  440\n",
      "i:  450\n",
      "i:  460\n",
      "i:  470\n",
      "if there is no output, means all 100 are incorrected, accurarcy = 0.\n",
      "end\n",
      "CPU times: user 969 ms, sys: 3.03 ms, total: 972 ms\n",
      "Wall time: 966 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "spell.distance = 1\n",
    "print(len(incorrect_words))\n",
    "mispelled = spell.unknown(incorrect_words)\n",
    "n_acc = 0\n",
    "n_total = 0\n",
    "for i, word in enumerate(mispelled):\n",
    "    n_total += 1\n",
    "    if i % 10 == 0:\n",
    "        print(\"i: \", i)\n",
    "    corrected = spell.correction(word)\n",
    "    if corrected == ground_truth_words[i]:\n",
    "        print(corrected)\n",
    "        n_acc += 1\n",
    "#     if spell.correction(word) is not None:\n",
    "#         print(spell.correction(word))\n",
    "print(\"if there is no output, means all 100 are incorrected, accurarcy = 0.\")        \n",
    "print(\"end\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.0 \n"
     ]
    }
   ],
   "source": [
    "print(f\"accuracy: {n_acc/n_total} \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we used pre-loaded database in the SpellChecker, the accuracy would be very low(acc = 0) because pre-loaded database is not tailored to clinical field. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add drug name dictionary to pyspellchecker package. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "drug list: 5583\n"
     ]
    }
   ],
   "source": [
    "print(\"drug list:\", len(lst))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "spell.word_frequency.load_words(lst)\n",
    "spell.distance = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to ensure that dataset is correctly stored here\n",
    "print(len(Unsprv_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "incorrect_words=[x[0] for x in Unsprv_dataset[:500]]\n",
    "ground_truth_words=[x[1] for x in Unsprv_dataset[:500]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500\n",
      "accuracy is   0.00\n",
      "end\n",
      "CPU times: user 948 ms, sys: 1.99 ms, total: 950 ms\n",
      "Wall time: 948 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print(len(incorrect_words))\n",
    "mispelled = spell.unknown(incorrect_words)\n",
    "n_acc = 0\n",
    "n_count = 0\n",
    "for i, word in enumerate(mispelled):\n",
    "    n_count += 1\n",
    "    corrected = spell.correction(word)\n",
    "    if corrected == ground_truth_words[i]:\n",
    "        n_acc +=1\n",
    "#     if spell.correction(word) is not None:\n",
    "#         print(spell.correction(word))\n",
    "print(f\"accuracy is {n_acc/n_count: 6.2f}\")     \n",
    "print(\"end\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "//sometimes, this part is working. \n",
    "If we add drug name to pyspellchecker dictionary, accuracy is 0.12 for 1000 tests and accuracy would drop to lower number. \n",
    "\n",
    "500 words: acc: 0.26\n",
    "1000 words: acc of 0.12\n",
    "5000 words: acc of 0.03\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['rytary', 'tenofovir alafenamide fumarate', 'serzone', 'butenafine hydrochloride', 'numorphan', 'hydromox', 'pioglitazone', 'betapace af', 'rezipas', 'sectral']\n",
      "['rytary', 'tenofovir', 'serzone', 'butenafine', 'numorphan', 'hydromox', 'pioglitazone', 'betapace', 'rezipas', 'sectral']\n",
      "['tenofovir alafenamide fumarate', 'rytary', 'serzone', 'butenafine hydrochloride', 'numorphan', 'hydromox', 'pioglitazone', 'betapace af', 'rezipas', 'sectral']\n",
      "updated list:  6394\n"
     ]
    }
   ],
   "source": [
    "print(lst[:10])\n",
    "uniword = [w.split()[0] for w in lst]  \n",
    "print(uniword[:10])\n",
    "updated_lst = list(set(lst+uniword))\n",
    "print(updated_lst[:10])\n",
    "print(\"updated list: \", len(updated_lst))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "lst = updated_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "drugset = set(lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.metrics.distance  import edit_distance\n",
    "\n",
    "\n",
    "import nltk\n",
    "from nltk.metrics.distance import jaccard_distance\n",
    "from nltk.util import ngrams\n",
    "\n",
    "\n",
    "def WordLenDiff(w1, w2):\n",
    "    return abs(len(w1) - len(w2))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reglan\n"
     ]
    }
   ],
   "source": [
    "correct_words = lst\n",
    "def edit_distance_rec(word):\n",
    "    # loop for finding correct spellings\n",
    "    # based on jaccard distance\n",
    "    # and printing the correct word\n",
    "    temp = [(edit_distance(word, w),w) for w in correct_words if WordLenDiff(w, word) <=3]\n",
    "    return sorted(temp, key = lambda val:val[0])[0][1]\n",
    "\n",
    "print(edit_distance_rec('reeglan'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reglan\n"
     ]
    }
   ],
   "source": [
    "def jaccard_distance_rec(word):\n",
    "    temp = [(jaccard_distance(set(ngrams(word, 2)),\n",
    "                              set(ngrams(w, 2))),w)\n",
    "            for w in correct_words if WordLenDiff(w, word) <=3]\n",
    "    return sorted(temp, key = lambda val:val[0])[0][1]\n",
    "\n",
    "print(jaccard_distance_rec('reeglan'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2 µs, sys: 1 µs, total: 3 µs\n",
      "Wall time: 6.2 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "def drug_inquiry(input_drug):\n",
    "    drugset = set(lst)\n",
    "    print(f\"\\nYour entered {input_drug}\")\n",
    "\n",
    "    if input_drug.lower() in drugset:\n",
    "        print(f\"{input_drug} is found in drug dictionary. \")\n",
    "    else:\n",
    "        print(f\"\\n{input_drug} is not spelled correctly\")\n",
    "        print(f\"Recommended drug spelling using Edit distance: {edit_distance_rec(input_drug)}\")\n",
    "        print(f\"Recommended drug spelling using Jaccard distance: {jaccard_distance_rec(input_drug)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Demo of simple inquiry system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Please enter drug name or enter STOP to exit: atorvastatin\n",
      "\n",
      "Your entered atorvastatin\n",
      "atorvastatin is found in drug dictionary. \n",
      "\n",
      "Please enter drug name or enter STOP to exit: atorvasttin\n",
      "\n",
      "Your entered atorvasttin\n",
      "\n",
      "atorvasttin is not spelled correctly\n",
      "Recommended drug spelling using edit distance: atorvastatin\n",
      "Recommended drug spelling using jaccard distance: atorvastatin\n",
      "\n",
      "Please enter drug name or enter STOP to exit: stop\n",
      "\n",
      "Exit the drug inquiry system. \n",
      "CPU times: user 359 ms, sys: 27.5 ms, total: 387 ms\n",
      "Wall time: 22.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "while True:\n",
    "    input_drug = input(\"\\nPlease enter drug name or enter STOP to exit: \")\n",
    "    if input_drug.lower() == \"stop\":\n",
    "        print(\"\\nExit the drug inquiry system. \")\n",
    "        break;\n",
    "    else:\n",
    "        drug_inquiry(input_drug)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Please enter drug name or enter STOP to exit: \n",
      "stop\n",
      "\n",
      "Exit the drug inquiry system. \n",
      "CPU times: user 1.07 s, sys: 371 ms, total: 1.44 s\n",
      "Wall time: 5min 5s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "while True:\n",
    "    input_drug = input(\"\\nPlease enter drug name or enter STOP to exit: \\n\")\n",
    "    if input_drug.lower() == \"stop\":\n",
    "        print(\"\\nExit the drug inquiry system. \")\n",
    "        break;\n",
    "    else:\n",
    "        drug_inquiry(input_drug)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This concludes the section for Qinglu Ren's part. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:nlp2021]",
   "language": "python",
   "name": "conda-env-nlp2021-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
